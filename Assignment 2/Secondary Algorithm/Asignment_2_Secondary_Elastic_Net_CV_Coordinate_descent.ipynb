{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asignment 2 - Secondary Elastic Net CV-Coordinate descent.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfJYEoZT0uZG"
      },
      "source": [
        "# **Assignment 2 Manual (From Scratch)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ly0Ym7-0vxC"
      },
      "source": [
        "- **Programmers:**\n",
        "  - Shaun Pritchard\n",
        "  - Ismael A Lopez\n",
        "- **Date:** 11-8-2021\n",
        "- **Assignment:** 2\n",
        "- **Prof:** M.DeGiorgio\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Overview: Secondary - Assignment 2**\n",
        "\n",
        "We analyzed the credit card data from N=400 training observations that you examined in Programming Assignment 1 using a penalized (regularized) least squares fit of a linear model using elastic net, with model parameters obtained by coordinate descent. \n",
        "\n",
        "Initially, we each worked independently, then we collaborated afterwards to finalize the assignment deliverables. This resulted in the completion of 2 methods for achieving the same goal, namely implementing ElastNet with coordinate descent. This is the second take on assignment 2. The aim was to display different methods of achieving the same abstraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOgaFwdekNsd"
      },
      "source": [
        "## **Import data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yBHZwnwjoZT"
      },
      "source": [
        "#Math libs\n",
        "from math import sqrt\n",
        "from scipy import stats\n",
        "import os\n",
        "# Data Science libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Graphics libs\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#Timers\n",
        "!pip install pytictoc\n",
        "from pytictoc import TicToc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJp9HKx_udBj"
      },
      "source": [
        "## **Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOi-sBx0j8qm"
      },
      "source": [
        "# Import Data\n",
        "df = pd.read_csv('/content/Credit_N400_p9.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXfExwHBj8xh"
      },
      "source": [
        "# Validate data import\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C33uztDMkU_c"
      },
      "source": [
        "## **Data Pre Proccessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DatKXUT34Jkr"
      },
      "source": [
        "# Assign dummy variables to catigorical feature attributes\n",
        "df = df.replace({'Male': 0, 'Female':1, 'No': 0, 'Yes': 1})\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BevwL9qB4NtL"
      },
      "source": [
        "# separate the predictors from the response\n",
        "X = df.to_numpy()[:, :-1]\n",
        "Y = df.to_numpy()[:, -1]\n",
        "print('Convert dataframe to numpy array:', X.shape, Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s1rQXpJjLLT"
      },
      "source": [
        "## **Set Global Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA_7cIsrosCE"
      },
      "source": [
        "# Set local variables\n",
        "# 9-Tuning Parms\n",
        "λ  = [1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
        "\n",
        "# 6 learning & convergence rate\n",
        "α =  [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "# K-folds\n",
        "k = 5\n",
        "\n",
        "#Iterations\n",
        "n_iters = 1000 # itterations\n",
        "\n",
        "#log base of lambda\n",
        "λ_log = np.log10(λ) \n",
        "\n",
        "# Set verbose to True\n",
        "verbose = True\n",
        "\n",
        "# Set n x m matrix variable\n",
        "X_p = X\n",
        "\n",
        "# Set n vector variable\n",
        "Y_p  = Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMnL5CwBjPhm"
      },
      "source": [
        "## **Instantiate Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynQQ1wIN-SXl"
      },
      "source": [
        "# Randomize N x M and N data\n",
        "def randomize_data(X_p, Y_p):\n",
        "  matrix = np.concatenate((X_p, Y_p[:, None]), 1)\n",
        "  np.random.shuffle(matrix)\n",
        "  return matrix[:, :-1], matrix[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7KiNOOxosHO"
      },
      "source": [
        "# Initilize random sample data\n",
        "x, y = randomize_data(X_p, Y_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zUhIYQVjx4x"
      },
      "source": [
        "# Set Global variable for samples and number of X = N X M features\n",
        "X1 = x.shape[0]\n",
        "X2 = x.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_bpEMBlWCg0"
      },
      "source": [
        "# Create a 𝛽 matrix to store the predictors \n",
        "𝛽 = np.zeros([k, len(λ), len(α), X2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMQG63nQV1UQ"
      },
      "source": [
        "# Store 5 K-fold cross validation results \n",
        "CV = np.zeros([k, len(λ), len(α)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAgvJF3KV1bv"
      },
      "source": [
        "# Compute the number of validation test samples and indices  based on k-folds \n",
        "test_x = X1 // k \n",
        "test_i = list(range(0, X1, test_x))\n",
        "        \n",
        "if True:\n",
        "    print('Implemnting {} training of {} test validation samples for each 5-k CV fold.'.format(\n",
        "        X1 - test_x, test_x)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZkDFhL6kScT"
      },
      "source": [
        "## **Implment Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QleECgSIosTw"
      },
      "source": [
        "# Standardize  X\n",
        "def standardize(x, mean_x, std_x):\n",
        "  return (x - mean_x) / std_x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJz8vo3IAV8u"
      },
      "source": [
        "# Center response variables\n",
        "def centerResponses(y, mean):\n",
        "  return y - mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3xgYvv-AWCc"
      },
      "source": [
        "# predicit x\n",
        "def predict(x):\n",
        "  x = standardize(x, mean_x, std_x)\n",
        "  return np.matmul(x, 𝛽x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olIs3_q64fel"
      },
      "source": [
        "# Calculate MSE score \n",
        "def score(x_test, y_test,  𝛽x):\n",
        "  ŷ = np.matmul(x_test, 𝛽x)\n",
        "  mse = np.mean((y_test - ŷ) ** 2)\n",
        "  return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7juI0vRtuxg6"
      },
      "source": [
        "### **Coordinate Descent Algortihm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvKcBJVAWHJ"
      },
      "source": [
        "# implement Coordinate Descent\n",
        "def coordinateDescent(x, y, 𝛽x, sum_sq, lamb, alpha):\n",
        "  for k in range(X2):\n",
        "    # RSS minus the k coefficient \n",
        "    RSS = y - np.matmul(x, 𝛽x) + (x[:, k] * 𝛽x[k])[:, None]\n",
        "            \n",
        "    # Calualte the RSS Loss function\n",
        "    a_k = np.matmul(x[:, k].T, RSS)[0]\n",
        "            \n",
        "    # update B_k\n",
        "    𝛽k = np.absolute(a_k) - lamb * (1 - alpha) / 2\n",
        "    𝛽k = 𝛽k if 𝛽k >= 0 else 0\n",
        "    𝛽x[k, 0] = np.sign(a_k) * 𝛽k / (sum_sq[k] + lamb * alpha)\n",
        "\n",
        "  return 𝛽x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBl1vzBu7p9"
      },
      "source": [
        "### **Elastic Net - Cross Validation Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YLzdlcirWAv"
      },
      "source": [
        "for i_lambda, lamb in enumerate(λ): # loop through λ lambda\n",
        "    for i_alpha, alpha in enumerate(α): # loop through α\n",
        "        for i_fold, i_test in zip(range(k), test_i): # loop through folds\n",
        "\n",
        "            # Validates and trains the CV iteration based on the validation and training sets.     \n",
        "            x_test = x[i_test:i_test + test_x]\n",
        "            x_train = np.delete(x, np.arange(i_test, i_test + test_x), axis = 0)\n",
        "            y_test = y[i_test:i_test + test_x]\n",
        "            y_train = np.delete(y, np.arange(i_test, i_test + test_x), axis = 0)\n",
        "\n",
        "\n",
        "            # Standardize x and center y  5 K-fold trianing and test data\n",
        "            mean_x, std_x = np.mean(x_train, 0), np.std(x_train, 0)\n",
        "            mean_res = np.mean(y_train)\n",
        "\n",
        "            # X training and test\n",
        "            x_train = standardize(x_train, mean_x, std_x)\n",
        "            x_test = standardize(x_test, mean_x, std_x)\n",
        "             \n",
        "            # Y training and test \n",
        "            y_train = centerResponses(y_train, mean_res)[:, None]\n",
        "            y_test = centerResponses(y_test, mean_res)[:, None]\n",
        "                      \n",
        "            # compute b_k given this fold \n",
        "            sum_sq = np.sum(x_train ** 2, 0)\n",
        "          \n",
        "\n",
        "            # initialize random Beta for this lambda and fold\n",
        "            𝛽x = np.random.uniform(low = -1, high = 1, size = (X2, 1))\n",
        "\n",
        "            # Iterate 1000 times through the beta values in Elastic Net algorithm\n",
        "            for iter in range(n_iters):\n",
        "                𝛽x = coordinateDescent(x_train, y_train, 𝛽x, sum_sq, lamb, alpha)\n",
        "            \n",
        "            # Calulate MSE score for the model -- \n",
        "            # TODO: issue with MSE calualtion\n",
        "            mse_score = score(x_test, y_test,  𝛽x)    \n",
        "             \n",
        "            # store the score with the tuning param combinations\n",
        "            CV[i_fold, i_lambda, i_alpha] = mse_score\n",
        "\n",
        "\n",
        "            # Store the coefficient vector\n",
        "            𝛽[i_fold, i_lambda, i_alpha] = 𝛽x[:, 0]\n",
        "\n",
        "        # Print out the mean CV MSE for lambda and alpha\n",
        "        if verbose:\n",
        "            print('lambda:{}; alpha:{}; CV MSE:{}'.format(lamb, alpha, np.mean(CV[:, i_lambda, i_alpha])))\n",
        "                \n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MItLPFP51szd"
      },
      "source": [
        "### **Test bench for parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex-iM1LO1sDx"
      },
      "source": [
        "print(sum_sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj47xJha1sLF"
      },
      "source": [
        "print(𝛽x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiUJpD4X1sQN"
      },
      "source": [
        "print(mse_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOrksjXIzMuH"
      },
      "source": [
        "### **Retrain data with optimal lambda and alpha**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdGC4K4ozMDn"
      },
      "source": [
        "#Retrain using all data with optimum lambda and alpha\n",
        "\n",
        "#Calculate CV mean    \n",
        "cv_mean = np.mean(CV, 0)\n",
        "\n",
        "# find the best lambda and alpha  \n",
        "best_λ_ind, best_alpha_index = np.where(cv_mean == np.amin(cv_mean))\n",
        "best_λ = λ[best_λ_ind[0]]\n",
        "best_alpha = α[best_alpha_index[0]]\n",
        "\n",
        "\n",
        "# standardize features of x and center responses \n",
        "mean_x, std_x = np.mean(x, 0), np.std(x, 0)\n",
        "x = standardize(x, mean_x, std_x)\n",
        "y = centerResponses(y, np.mean(y))[:, None]\n",
        "                                    \n",
        "\n",
        "# Compute the sum of squares for each feature on the entire dataset\n",
        "sum_sq = np.sum(x ** 2, 0)\n",
        "                                    \n",
        "# initialize 𝛽x coefficients\n",
        "𝛽x = np.random.uniform(low = -1, high = 1, size = (X2, 1))\n",
        "\n",
        "# Run coordent decent algorithm for best MSE \n",
        "for iter in range(n_iters):\n",
        "    𝛽x = coordinateDescent(x, y, 𝛽x, sum_sq, best_λ, best_alpha)\n",
        "    # print('Beta values updated test:',𝛽x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty8Clv160Can"
      },
      "source": [
        "### **Test lambda and alpha**\n",
        "\n",
        "- Best lambda shouild be 1.0\n",
        "- Best alpha should be 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX87BLEuzwUl"
      },
      "source": [
        "print('Best lambda:=', best_λ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaO7_Zdqz7pk"
      },
      "source": [
        "print('Best alpha:=', best_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiOvMc09zjdE"
      },
      "source": [
        "### **Output for Deliverable 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndSBpD-1CrvA"
      },
      "source": [
        "# For each alpha, the coefficient values are plotted over the five folds as a function of lambda.\n",
        "sns.set_theme()\n",
        "sns.set_style(\"darkgrid\", {\"grid.color\": \".5\", \"grid.linestyle\": \":\" })\n",
        "𝛽μ  = np.mean(𝛽,0)\n",
        "ŷ = df.columns\n",
        "count = 0 # set itterator\n",
        "t = TicToc()  # measure time of convergance\n",
        "# create instance of class\n",
        "for i_alpha, alpha in enumerate(α):\n",
        "    count += 1 \n",
        "    end_time = t.toc()\n",
        "    plt.figure()\n",
        "    plt.figure(figsize=(16, 10), dpi=70)\n",
        "    print('Tuning parameter converged at = #{c} λ {} at alpha{α}\\n'.format(np.log10(λ), c=count,  α=alpha)) \n",
        "    for i_beta in range(𝛽μ.shape[1]):\n",
        "        plt.plot( np.log10(λ), 𝛽μ[:, i_alpha, i_beta], label = ŷ[i_beta])\n",
        "    plt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper right', title = 'Features')\n",
        "    plt.xlabel('λ Tuning Params')\n",
        "    plt.ylabel('Coefficient Values')\n",
        "    plt.title('Alpha Value: {}'.format(alpha))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xEn-WB4-Jsw"
      },
      "source": [
        "### **Output for Deliverable 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Ju28nKt6yD"
      },
      "source": [
        "# Observe the CV MSE over values of lambda and alpha\n",
        "plt.figure()\n",
        "plt.figure(figsize=(16, 10), dpi=70)\n",
        "for i_alpha, alpha in enumerate(α):\n",
        "    std_error = np.std(CV[..., i_alpha], 0) / np.sqrt(k)\n",
        "    plt.errorbar( np.log10(λ), np.mean(CV[..., i_alpha], 0), yerr = std_error,xuplims=True,label = str(alpha))\n",
        "    plt.xlabel('Log base lambda')\n",
        "    plt.ylabel('Cross Validation MSE')\n",
        "    plt.legend(title = 'α')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0-7CMo--LvE"
      },
      "source": [
        "### **Output for Deliverable 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC5Hs87Mt61h"
      },
      "source": [
        "# lambda and alpha with lowest cv mse\n",
        "print('Best lambda: {}; Best alpha: {}'.format(best_λ, best_alpha))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpFGTQox-NsW"
      },
      "source": [
        "### **Output for Deliverable 4.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxqhfVdAt65Q"
      },
      "source": [
        "# Lasso Implentation\n",
        "# Plot the coefficient vectors for optimal lambda given alpha = 0 \n",
        "# MSE cross valiadtion for alpha = 0\n",
        "alpha0 = np.mean(CV[..., 0], 0)\n",
        "\n",
        "# Get the index of lambda with lowest CV MSE\n",
        "index = np.argmin(alpha0)\n",
        "λx = λ[index]\n",
        "\n",
        "# CV 5 K-fold mean vector coefficient for lambda and alpha\n",
        "𝛽μ = np.mean(𝛽[:, index, 0, :], 0)\n",
        "\n",
        "# Calulate and plot optimal lambda and alpha values\n",
        "plt.figure()\n",
        "plt.figure(figsize=(16, 10), dpi=70)\n",
        "plt.scatter(𝛽x, 𝛽μ)\n",
        "plt.plot(np.arange(-300, 475), np.arange(-300, 475), '--', color = 'g')\n",
        "plt.xlabel('Elastic Net (lambda = {}, alpha = {})'.format(best_λ, best_alpha))\n",
        "plt.ylabel('|L2|:= (lambda = {})'.format(λx))\n",
        "plt.show()\n",
        "\n",
        "####### Test #######\n",
        "# print(alpha0)\n",
        "# print(λ_i)\n",
        "# print(𝛽μ )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgO6TtR-Pvg"
      },
      "source": [
        "### **Output for Deliverable 4.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orTqQy_Pt672"
      },
      "source": [
        "# Ridge Implentation\n",
        "# Plot the coefficient vectors for optimal lambda given alpha = 0\n",
        "\n",
        "# get cv mse given alpha = 1\n",
        "alpha1 = np.mean(CV[..., -1], 0)\n",
        "\n",
        "# Get the index of lambda with lowest CV MSE\n",
        "λ_i = np.argmin(alpha1)\n",
        "λx  = λ[λ_i]\n",
        "\n",
        "# CV 5 K-fold mean vector coefficient for lambda and alpha\n",
        "𝛽μ = np.mean(𝛽[:, λ_i, -1, :], 0)\n",
        "\n",
        "# Calulate and plot optimal lambda and alpha values\n",
        "plt.figure()\n",
        "plt.figure(figsize=(16, 10), dpi=70)\n",
        "plt.scatter(𝛽x, 𝛽μ)\n",
        "plt.plot(np.arange(-300, 475), np.arange(-300, 475), '--', color = 'b')\n",
        "plt.xlabel('Elastic Net (lambda = {}, alpha = {})'.format(best_λ, best_alpha))\n",
        "plt.ylabel('L2 (lambda = {})'.format(λx))\n",
        "plt.show()\n",
        "\n",
        "####### Test #######\n",
        "# print(alpha1)\n",
        "# print(λ_i)\n",
        "# print(𝛽μ )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ6NvrEpoFkz"
      },
      "source": [
        "# **Assignment 2 with ML libaries**\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKkgkr_3AeeY"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "#from sklearn.utils._testing import ignore_warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning) # To filter out the Convergence warning\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Scaling LibrariesL\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import packages for Measuring Model Perormance\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "\n",
        "from itertools import product\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t46GVsGPEYBR"
      },
      "source": [
        "# Target:\n",
        "y= df['Balance'].to_numpy()\n",
        "\n",
        "# Convert the Pandas dataframe to numpy ndarray for computational improvement\n",
        "X = df.iloc[:,:-1]\n",
        "X = X.to_numpy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t32JWW61BPPN"
      },
      "source": [
        "# Define my tuning parameter values 𝜆:\n",
        "\n",
        "learning_rates_λ = [1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
        "# Define my L1 Ratio:\n",
        "l1_ratio = [0, 1/5, 2/5, 3/5, 4/5, 1]\n",
        "\n",
        "# Define our tunning rates\n",
        "tuning_params = list(product(l1_ratio, learning_rates_λ))\n",
        "tuning_params[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z62DytFAvOw"
      },
      "source": [
        "# Create Standarizing ObjectPackages:\n",
        "standardization = StandardScaler()\n",
        "\n",
        "# Strandardize \n",
        "n_observations = len(df)\n",
        "variables = df.columns\n",
        "\n",
        "\n",
        "# Standardize the Predictors (X)\n",
        "Xst = standardization.fit_transform(X)\n",
        "\n",
        "# Add a constanct to the predictor matrix\n",
        "#Xst = np.column_stack((np.ones(n_observations),Xst))\n",
        "\n",
        "\n",
        "# Save the original M and Std of the original data. Used for unstandardize\n",
        "original_means = standardization.mean_\n",
        "\n",
        "# we chanced standardization.std_ to standardization.var_**.5\n",
        "originanal_stds = standardization.var_**.5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"observations :\", n_observations)\n",
        "print(\"variables :\", variables[:2])\n",
        "print('original_means :', original_means)\n",
        "print('originanal_stds :', originanal_stds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVazkEntA-Xw"
      },
      "source": [
        "# Center y not using a library:\n",
        "y_Mean = y.mean(axis = 0) # Original y mean\n",
        "\n",
        "y_Centered =  y-y_Mean\n",
        "\n",
        "print('Original y: ',y[:3])\n",
        "print(\"mean of y :\", y_Mean, \"Std of y :\", y.std(axis = 0))\n",
        "\n",
        "print('Centered y: ',y_Centered[:3])\n",
        "print(\"mean of y centered :\", y_Centered.mean(axis = 0), \"Std of y centered :\", y_Centered.std(axis = 0))\n",
        "\n",
        "print(y_Centered.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6i2QjXWB4Oh"
      },
      "source": [
        "#let's first split it into train and test part\n",
        "X_train, X_out_sample, y_train, y_out_sample = train_test_split(Xst, y_Centered, test_size=0.30, random_state=101) # Training and testing split\n",
        "\n",
        "# Print Data size\n",
        "print (\"Train dataset sample size: {}\".format(len(X_train)))\n",
        "print (\"Test dataset sample size: {}\".format(len(X_out_sample)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFP45zO3BC7R"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "L𝛽_per_λ=[] # set empty list\n",
        "\n",
        "# Evaluate tuning parameters with Elastic Net penalty\n",
        "for tuning_param in tuning_params:\n",
        "        Library_ElasticNet=ElasticNet(alpha=tuning_param[1] , l1_ratio= tuning_param[0], max_iter=5000, tol=0.01)\n",
        "        Library_ElasticNet.fit(X_train, y_train)\n",
        "        c = np.array(Library_ElasticNet.coef_)\n",
        "        c = np.append(tuning_param[1],c)\n",
        "        c = np.append(tuning_param[0],c)\n",
        "        L𝛽_per_λ.append(c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OjWvaK1BKYY"
      },
      "source": [
        "TunnedL𝛽_df=pd.DataFrame(L𝛽_per_λ)\n",
        "TunnedL𝛽_df.columns=['Alpha', 'Lamba','Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married']\n",
        "TunnedL𝛽_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVodBaIDFMmx"
      },
      "source": [
        "## **Deliverable 6.1**  <a class=\"anchor\" id=\"Deliverable_6.1\"></a>\n",
        "<h>\n",
        "\n",
        "> Illustrate the effect of the tuning parameter on the inferred elastic net regression coefficients by generating six plots (one for each 𝛼 value) of nine lines (one for each of the 𝑝=9 features), with the 𝑦-axis as 𝛽̂\n",
        "𝑗, 𝑗=1,2,…,9, and the 𝑥-axis the corresponding log-scaled tuning parameter value log10(𝜆) that generated the particular 𝛽̂\n",
        "𝑗."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILUup773CC74"
      },
      "source": [
        "plt.figure(figsize=(30,30))\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 0')\n",
        "plt.ylabel('Standardize Coefficients')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.2)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.2)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 0.2')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.4)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.4)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 0.4')\n",
        "plt.ylabel('Standardize Coefficients')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.6)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.6)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 0.6')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 5)\n",
        "\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.8)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(0.8)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 0.8')\n",
        "plt.xlabel('Learning Rates λ')\n",
        "plt.ylabel('Standardize Coefficients')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 6)\n",
        "\n",
        "plt.plot(TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(1)].iloc[:,1:2],TunnedL𝛽_df[TunnedL𝛽_df.Alpha.eq(1)].iloc[:,2:])\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients alpha = 1')\n",
        "plt.xlabel('Learning Rates λ')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='best')\n",
        "plt.legend(TunnedL𝛽_df.columns[2:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6FieYOtFHrq"
      },
      "source": [
        "# **Deliverable 6.2**  <a class=\"anchor\" id=\"Deliverable_6.2\"></a>\n",
        "Illustrate the effect of the tuning parameters on the cross validation error by generating a plot of six lines (one for each l1_ratio value) with the y-axis as \n",
        "CV(5) error, and the x-axis the corresponding log-scaled tuning parameter value log10(λ) that generated the particular CV(5) error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVRg652CbhI"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "#Define the model\n",
        "Library_ElasticNet = ElasticNet()\n",
        "\n",
        "# Create the Kfold:\n",
        "cv_iterator = KFold(n_splits = 5, shuffle=True, random_state=101)\n",
        "cv_score = cross_val_score(Library_ElasticNet, Xst, y_Centered, cv=cv_iterator, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "#print (cv_score)\n",
        "#print ('Cv score: mean %0.3f std %0.3f' % (np.mean(np.abs(cv_score)), np.std(cv_score))) \n",
        "\n",
        "# define grid\n",
        "Parm_grid = dict()\n",
        "Parm_grid['alpha'] = learning_rates_λ\n",
        "Parm_grid['l1_ratio'] = l1_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNMsVg-zFRD2"
      },
      "source": [
        "# Lets define search\n",
        "GsearchCV = GridSearchCV(estimator = Library_ElasticNet, param_grid = Parm_grid, scoring = 'neg_mean_absolute_error', n_jobs=1, refit=True, cv=cv_iterator)\n",
        "GsearchCV.fit(Xst, y_Centered)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDcdvB6OFTDu"
      },
      "source": [
        "GCV_df = pd.concat([pd.DataFrame(GsearchCV.cv_results_[\"params\"]),pd.DataFrame(GsearchCV.cv_results_[\"mean_test_score\"], columns=[\"mean_test_score\"])],axis=1)\n",
        "#GCV_df.index=GCV_df['alpha']\n",
        "\n",
        "GCV_df[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPCoNuchFWP2"
      },
      "source": [
        "plt.figure(figsize=(25/2.54,15/2.54))\n",
        "\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(0)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0)][\"mean_test_score\"] ), label='Avg_MSE_T alpha = 0')\n",
        "\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.2)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.2)][\"mean_test_score\"]), label='Avg_MSE_T alpha = 0.2')\n",
        "\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.4)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.4)][\"mean_test_score\"]), label='Avg_MSE_T alpha = 0.4')\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.6)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.6)][\"mean_test_score\"]), label='Avg_MSE_T alpha = 0.6')\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.8)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.8)][\"mean_test_score\"]), label='Avg_MSE_T alpha = 0.8')\n",
        "plt.plot( GCV_df[GCV_df.l1_ratio.eq(1)][\"alpha\"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(1)][\"mean_test_score\"]), label='Avg_MSE_T alpha = 1')\n",
        "\n",
        "\n",
        "plt.title('Effect of tunning on Coefficients')\n",
        "plt.xlabel('Learning Rates λ')\n",
        "plt.ylabel('Cross Validation MSE')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2rVYRLFaub"
      },
      "source": [
        "# **Deliverable 6.3**  <a class=\"anchor\" id=\"Deliverable_6.3\"></a>\n",
        "Indicate the value of 𝜆 that generated the smallest CV(5) error\n",
        "\n",
        "**Smallest CV with Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIGta1BxFXxl"
      },
      "source": [
        "print ('Best: ',GsearchCV.best_params_)\n",
        "print ('Best CV mean squared error: %0.3f' % np.abs(GsearchCV.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T9dMw0mFZ_m"
      },
      "source": [
        "GCV_df.sort_values(by=['mean_test_score'], ascending=False)[:1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RAR44sWFmyY"
      },
      "source": [
        "# Alternative: sklearn.linear_model.ElasticNetCV\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "\n",
        "auto_EN = ElasticNetCV(alphas=learning_rates_λ, l1_ratio = l1_ratio, normalize=False,  n_jobs=1,  cv=cv_iterator)\n",
        "auto_EN.fit(Xst, y_Centered)\n",
        "print ('Best alpha: %0.5f' % auto_EN.alpha_)\n",
        "print ('Best L1 ratio: %0.5f' % auto_EN.l1_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlpvkBRFrhC"
      },
      "source": [
        "# **Deliverable 6.4**  <a class=\"anchor\" id=\"Deliverable_6.4\"></a>\n",
        "Given the optimal 𝜆, retrain your model on the entire dataset of 𝑁=400 observations and provide the estimates of the 𝑝=9 best-fit model parameters.\n",
        "\n",
        "\n",
        "**Tunned with best alpha with Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQcfdo_FoZ2"
      },
      "source": [
        "Library_ElasticNet_best=ElasticNet(alpha=GsearchCV.best_params_['alpha'] , l1_ratio= GsearchCV.best_params_['l1_ratio'], max_iter=1000, tol=0.1)\n",
        "Library_ElasticNet_best.fit( Xst, y_Centered )\n",
        "\n",
        "y_predM_best = Library_ElasticNet_best.predict(X_out_sample)\n",
        "print (\"Betas= \", Library_ElasticNet_best.coef_)\n",
        "\n",
        "print(\"MSE = \",mean_squared_error(y_out_sample, y_predM_best))\n",
        "print('R^2 Test', r2_score(y_out_sample, y_predM_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW0RSxKlGqUm"
      },
      "source": [
        "**lasso (𝛼=0 under optimal 𝜆 for 𝛼 =0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zY788zGAUV"
      },
      "source": [
        "Library_ElasticNet_best=ElasticNet(alpha=GsearchCV.best_params_['alpha'] , l1_ratio= 0, max_iter=1000, tol=0.1)\n",
        "Library_ElasticNet_best.fit( Xst, y_Centered )\n",
        "\n",
        "y_predM_best = Library_ElasticNet_best.predict(X_out_sample)\n",
        "print (\"Betas= \", Library_ElasticNet_best.coef_)\n",
        "\n",
        "print(\"MSE = \",mean_squared_error(y_out_sample, y_predM_best))\n",
        "print('R^2 Test', r2_score(y_out_sample, y_predM_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhy5TB_sGZJU"
      },
      "source": [
        "**ridge regression (𝛼=1 under optimal 𝜆 for 𝛼=1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6gX56K9GUcV"
      },
      "source": [
        "Library_ElasticNet_best=ElasticNet(alpha=GsearchCV.best_params_['alpha'] , l1_ratio= 1, max_iter=1000, tol=0.1)\n",
        "Library_ElasticNet_best.fit( Xst, y_Centered )\n",
        "\n",
        "y_predM_best = Library_ElasticNet_best.predict(X_out_sample)\n",
        "print (\"Betas= \", Library_ElasticNet_best.coef_)\n",
        "\n",
        "print(\"MSE = \",mean_squared_error(y_out_sample, y_predM_best))\n",
        "print('R^2 Test', r2_score(y_out_sample, y_predM_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dPcrxtHQ_K"
      },
      "source": [
        "Comapring the modeles with the optimal alpha (λ) and replacing the l1ratio (α) between 0 (lasso) and 1 (ridge) we are able to see that our model is more of a ridge model. Which is validated by the best_score_ (l1 ratio) from our CV Gridserach. The coefficient that is highly optimize is the Rating feature. Which is telling as balance, risk, and credit is highly correlated to this feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L9GKGw-I8pW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}