# -*- coding: utf-8 -*-
"""Assignment 2 Primary - Elastic Net CV-Coordinate Descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ur5u_FGm5g4PkkwdpOazER2h3G67Rg5t

- **Programmer:**
    - **Shaun Pritchard**
    - **Ismael A Lopez** 

## **Assigment 2**
**Brief overview of assignment**

***You will perform a penalized (regularized) least squares fit of a linear model using elastic net, with the model parameters obtained by coordinate descent***

<hr>

### **Overview: Primary - Assignment 2**

We analyzed the credit card data from N=400 training observations that you examined in Programming Assignment 1 using a penalized (regularized) least squares fit of a linear model using elastic net, with model parameters obtained by coordinate descent. 

Initially, we each worked independently, then we collaborated afterwards to finalize the assignment deliverables. This resulted in the completion of 2 methods for achieving the same goal, namely implementing ElastNet with coordinate descent. This is the second take on assignment 2. The aim was to display different methods of achieving the same abstraction.

> ***Table of Contents***
<hr>


* [Import Packages](#Import_Packages)
    * [Import packages for manipulating data](#Import_packages_for_manipulating_data)
    * [Import packages for splitting data](#Import_packages_for_splitting_data)
    * [Import packages for modeling data](#Import_packages_for_modeling_data)
    * [Import packages for Scaling and Centering data](#Import_packages_for_Scaling_and_Centering_data)
    * [Import packages for Measuring Model Perormance](#Import_packages_for_Measuring_Model_Perormance)
    
* [Data Processing](#Data_Processing)
    * [Import Data](#Import_data)
    * [Lets change the categorical values](#Lets_change_the_categorical_values)
    * [Create Predictor and Target numpy array](#Create_Predictor_and_Target_numpy_array)
    * [Create a Normalize copy of variables](#Create_a_Normalize_copy_of_variables)
    * [Split Data](#Split_Data:)
* [Regression Model](#Regression_Model)
    * [Define our learning rates:](#Define_our_learning_rates)
    * [Create the Regression Objects](#Create_the_Regression_Objects)
        * [Elastic Net Manual](#Ridge_Regression_Manual)

        * [Elastic Net Library](#Ridge_Regression_Library)

> ***Deliverables***
<hr>

* [**Deliverable 6.1**](#Deliverable_6.1)
* [**Deliverable 6.2**](#Deliverable_6.2)
* [**Deliverable 6.3**](#Deliverable_6.3)
* [**Deliverable 6.4**](#Deliverable_6.4)
* [**Deliverable 6. Reason for difference**](#Deliverable_6_Reason_for_difference)

# Import Packages <a class="anchor" id="Import_Packages"></a>

### Import packages for manipulating data  <a class="anchor" id="Import_packages_for_manipulating_data"></a>
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.mlab as mlab
import math
import csv
import random
# %matplotlib inline

"""### Import packages for splitting data  <a class="anchor" id="Import_packages_for_splitting_data"></a>"""

from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold
from sklearn.model_selection import GridSearchCV

"""### Import packages for modeling data  <a class="anchor" id="Import_packages_for_modeling_data"></a>"""

# Import models:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression as linearR_Model, Ridge  as RidgeR_Model
from sklearn.linear_model import RidgeCV

from sklearn.linear_model import ElasticNet
from sklearn.linear_model import ElasticNetCV

from sklearn.exceptions import ConvergenceWarning
#from sklearn.utils._testing import ignore_warnings
import warnings
warnings.filterwarnings('ignore', category=ConvergenceWarning) # To filter out the Convergence warning
warnings.filterwarnings('ignore', category=UserWarning)
from itertools import product

"""### Import packages for Scaling and Centering data  <a class="anchor" id="Import_packages_for_Scaling_and_Centering_data"></a>"""

from sklearn.preprocessing import StandardScaler

"""### Import packages for Measuring Model Perormance  <a class="anchor" id="Import_packages_for_Measuring_Model_Perormance"></a>"""

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import make_scorer

"""# Data Processing <a class="anchor" id="Data_Processing"></a>

### Import Data  <a class="anchor" id="Import_data"></a>
"""

dataset = pd.read_csv ('Credit_N400_p9.csv')
dataset.head(3)

# What are the datatypes of each observation:
print(dataset.dtypes)
# Shape of my data
print('The size of our data are: ',dataset.shape)

# Are there any null or missing values
dataset.isnull().sum()

"""### Lets change the categorical values  <a class="anchor" id="Lets_change_the_categorical_values"></a>

"""

print("Unique Values for Genders: ", dataset.Gender.unique())
print("Unique Values for Student: ", dataset.Student.unique())
print("Unique Values for Married: ", dataset.Married.unique())

# Replace Gender to qualitative value
dataset['Gender'].replace({'Male':0, 'Female':1}, inplace=True)
# Replace Student status to qualitative value
dataset['Student'].replace({'No':0, 'Yes':1}, inplace=True)
# Replace Martial status to qualitative value
dataset['Married'].replace({'No':0, 'Yes':1}, inplace=True)

# Check our categorical change:
dataset.head(3)

"""### Create Predictor and Target numpy array  <a class="anchor" id="Create_Predictor_and_Target_numpy_array"></a>"""

# Target:
y= dataset['Balance'].to_numpy()
y[:3]

# Convert the Pandas dataframe to numpy ndarray for computational improvement
X = dataset.iloc[:,:-1]
X = X.to_numpy()

print(type(X), X[:1])

# Let's add the predictor to a dataframe and get some descriptors:
X_desc = pd.DataFrame(X, columns=dataset.columns[:-1])
X_desc.describe()

"""### Create a Normalize copy of variables <a class="anchor" id="Create_a_Normalize_copy_of_variables"></a>"""

# Create Standarizing ObjectPackages:
#standardization = StandardScaler()

# Strandardize 
n_observations = len(dataset)
variables = dataset.columns


# Standardize the Predictors (X)
#Xst = standardization.fit_transform(X)

# Add a constanct to the predictor matrix
#Xst = np.column_stack((np.ones(n_observations),Xst))


# Save the original M and Std of the original data. Used for unstandardize
#original_means = standardization.mean_

# we chanced standardization.std_ to standardization.var_**.5
#originanal_stds = standardization.var_**.5


def standardize(X):
    mean_x, std_x = np.mean(X, 0), np.std(X, 0)
    standardization = (X - mean_x) / std_x
    return  mean_x, std_x, standardization

original_means, originanal_stds, Xst = standardize(X)


print("observations :", n_observations)
print("variables :", variables[:2])
print('original_means :', original_means)
print('originanal_stds :', originanal_stds)

# Let's add the predictor to a dataframe and get some descriptors:
Xst_desc = pd.DataFrame(Xst, columns=dataset.columns[:-1])
Xst_desc.describe()

# Center y not using a library:
y_Mean = y.mean(axis = 0) # Original y mean

y_Centered =  y-y_Mean

print('Original y: ',y[:3])
print("mean of y :", y_Mean, "Std of y :", y.std(axis = 0))

print('Centered y: ',y_Centered[:3])
print("mean of y centered :", y_Centered.mean(axis = 0), "Std of y centered :", y_Centered.std(axis = 0))

print(y_Centered.shape)

"""### Split Data: <a class="anchor" id="Split_Data:"></a>"""

#let's first split it into train and test part
X_train, X_out_sample, y_train, y_out_sample = train_test_split(Xst, y_Centered, test_size=0.40, random_state=101) # Training and testing split

X_validation, X_test, y_validation, y_test = train_test_split(X_out_sample, y_out_sample, test_size=0.50, random_state=101) # Validation and test split

# Print Data size
print ("Train dataset sample size: {}".format(len(X_train)))
print ("Validation dataset sample size: {}".format(len(X_validation)))
print ("Test dataset sample size: {}".format( len(X_test)))

"""# Regression Model <a class="anchor" id="Regression_Model"></a>
<hr>

### Define our learning rates <a class="anchor" id="Define_our_learning_rates"></a>
"""

# Define my tuning parameter values ùúÜ:

learning_rates_Œª = [1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]
print(learning_rates_Œª)

# Define my L1 Ratio:
l1_ratio = [0, 1/5, 2/5, 3/5, 4/5, 1]
l1_ratio

tuning_params = list(product(l1_ratio, learning_rates_Œª))
tuning_params[:2]

"""### Create the Regression Objects <a class="anchor" id="Create_the_Regression_Objects"></a>

**Elastic Net Manual** <a class="anchor" id="Ridge_Regression_Manual"></a>
"""

def predict(X, w):
    return np.dot(X, w)



def coordinateDescent(X, y, alpha, l1_ratio, tol=1e-4, max_it = 1000, path_length = 5 ):

    b = np.sum(X ** 2, 0) # bùëò
    
    X_row , X_col = X.shape  

    ùõΩx = np.random.uniform(-1,1,X_col) 

    for i in range(max_it):
         while True:
            B_s = ùõΩx   
            for k in range(len(ùõΩx)):            
                a_k  = np.dot(X[:, k].T, np.add(y - np.dot(X, ùõΩx), np.multiply(ùõΩx[k], X[:, k])))
                ùõΩx[k] = (np.sign(a_k) * max(0, np.abs(a_k) - (alpha*(1-l1_ratio)/2)) / np.add(b[k], (alpha*l1_ratio)))
            if np.all(abs(B_s - ùõΩx) < tol): # break out of the while loop if the diff is smaller than the tolerance level
                break
    return ùõΩx

cord = coordinateDescent(Xst, y_Centered , 1 , 1) 

print ("Betas= ", cord)
y_predM = predict(Xst,cord)
print("MSE = ",mean_squared_error(y_Centered, y_predM))
print('R^2 Test', r2_score(y_Centered, y_predM))

"""**Elastic Net Library** <a class="anchor" id="Ridge_Regression_Library"></a>"""

# ElasticNet Regression
from sklearn.linear_model import ElasticNet
Library_ElasticNet = ElasticNet(alpha= 1 , l1_ratio= 1, max_iter=1000, tol=0.01)

Library_ElasticNet.fit(Xst,y_Centered)

y_predM = Library_ElasticNet.predict(Xst)
print ("Betas= ", Library_ElasticNet.coef_)
#print ("Intercept= ",Library_ElasticNet.intercept_)
print("MSE = ",mean_squared_error(y_Centered, y_predM))
print('R^2 Test', r2_score(y_Centered, y_predM))

"""## **Deliverable 6.1**  <a class="anchor" id="Deliverable_6.1"></a>
<h>

> Illustrate the effect of the tuning parameter on the inferred elastic net regression coefficients by generating six plots (one for each ùõº value) of nine lines (one for each of the ùëù=9 features), with the ùë¶-axis as ùõΩÃÇ
ùëó, ùëó=1,2,‚Ä¶,9, and the ùë•-axis the corresponding log-scaled tuning parameter value log10(ùúÜ) that generated the particular ùõΩÃÇ
ùëó.

### Manual:
"""

MùõΩ_per_Œª = []
tP = []
for tuning_param in tuning_params:
    cord = coordinateDescent(Xst, y_Centered , tuning_param[1] , tuning_param[0]) 

    MùõΩ_per_Œª.append(cord)
    tP.append((tuning_param[0], tuning_param[1]))

TunnedMùõΩ_df = pd.DataFrame( np.column_stack( (np.array(tP), np.array(MùõΩ_per_Œª)) ) )
TunnedMùõΩ_df.columns=['Alpha', 'Lamba','Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married']
TunnedMùõΩ_df.iloc[:5,:]

plt.figure(figsize=(30,30))

plt.subplot(4, 2, 1)
plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])


plt.subplot(4, 2, 2)
plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.2)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.2)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.2')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])


plt.subplot(4, 2, 3)

plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.4)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.4)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.4')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])


plt.subplot(4, 2, 4)

plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.6)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.6)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.6')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])


plt.subplot(4, 2, 5)

plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.8)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(0.8)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.8')
plt.xlabel('Learning Rates Œª')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])



plt.subplot(4, 2, 6)

plt.plot(TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(1)].iloc[:,1:2],TunnedMùõΩ_df[TunnedMùõΩ_df.Alpha.eq(1)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 1')
plt.xlabel('Learning Rates Œª')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedMùõΩ_df.columns[2:])

"""**Elastic Net with Library**"""

from sklearn.linear_model import ElasticNet

LùõΩ_per_Œª=[] # set empty list

# Evaluate tuning parameters with Elastic Net penalty
for tuning_param in tuning_params:
        Library_ElasticNet=ElasticNet(alpha=tuning_param[1] , l1_ratio= tuning_param[0], max_iter=5000, tol=0.01)
        Library_ElasticNet.fit(X_train, y_train)
        c = np.array(Library_ElasticNet.coef_)
        c = np.append(tuning_param[1],c)
        c = np.append(tuning_param[0],c)
        LùõΩ_per_Œª.append(c)

TunnedLùõΩ_df=pd.DataFrame(LùõΩ_per_Œª)
TunnedLùõΩ_df.columns=['Alpha', 'Lamba','Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married']
TunnedLùõΩ_df.head()

plt.figure(figsize=(30,30))

plt.subplot(4, 2, 1)
plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])


plt.subplot(4, 2, 2)
plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.2)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.2)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.2')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])


plt.subplot(4, 2, 3)

plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.4)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.4)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.4')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])


plt.subplot(4, 2, 4)

plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.6)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.6)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.6')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])


plt.subplot(4, 2, 5)

plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.8)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(0.8)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 0.8')
plt.xlabel('Learning Rates Œª')
plt.ylabel('Standardize Coefficients')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])



plt.subplot(4, 2, 6)

plt.plot(TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(1)].iloc[:,1:2],TunnedLùõΩ_df[TunnedLùõΩ_df.Alpha.eq(1)].iloc[:,2:])

plt.title('Effect of tunning on Coefficients alpha = 1')
plt.xlabel('Learning Rates Œª')

plt.xscale('log')
plt.legend(loc='best')
plt.legend(TunnedLùõΩ_df.columns[2:])

"""# **Deliverable 6.2**  <a class="anchor" id="Deliverable_6.2"></a>
Illustrate the effect of the tuning parameters on the cross validation error by generating a plot of six lines (one for each l1_ratio value) with the y-axis as 
CV(5) error, and the x-axis the corresponding log-scaled tuning parameter value log10(Œª) that generated the particular CV(5) error

**Manual CV Elastic Net Manual**
"""

def predict(X, w):
    return np.dot(X, w)
    
def get_folds(X, y, k=5):
    df = np.hstack((X, y)) # Combine X and y in the same row: use [hstack or column_stack] depending on dimensions of y
    indices = np.arange(len(y)) # get the index of the response (y) variable
    np.random.shuffle(indices) # randomly shuffle the indecies
    sets = np.array_split(indices, k) # Split the index by k-folds
    fn = lambda index: df[sets[index], :] # Index the df matrix with the y range (0:#)
    return [fn(i) for i in np.arange(5)] # Append a list of all the values to a list by k-folds

def get_loss(B,test_data):
    y_predicted = np.dot(test_data[:, 0:-1], B) + B[0] # call Predict Function
    error =  np.linalg.norm(test_data[:, -1] - y_predicted)**2
    return error
    
def get_loss2 (X, w):
    y_predicted = np.dot(X[:, 0:-1], w)
    mse = np.mean(np.power(X[:, -1] - y_predicted, 2))
    return mse

def elastic_net_GS (X, y, alpha , l1_ratio , k=5):
	alpha = np.array(alpha)
	l1_ratio = np.array(l1_ratio)

	tuning_params = list(product(alpha, l1_ratio))
	max_Parm_iteration=len(tuning_params)

	B_list = [] # list of betas
	errors = [] # error list
	
	folds = get_folds(X, y.reshape(-1,1), k) # calls the get_fold function to fold the data into k-folds
	X_CVfolds = [folds[i][:, 0:-1] for i in np.arange(k)] # gets all the columns for X in according to the K-folds and places them on a list
	y_CVfolds = [folds[i][:, [-1]] for i in np.arange(k)] # gets y column in according to the K-folds and places them on a list
	fold_index = np.arange(k) # generate a np.ndarray with range from 0:k for the loop

	for i in fold_index: # loop from range
			X_test_CVfolds, y_test_CVfolds = X_CVfolds[i], y_CVfolds[i] # calls the X and Y fold according to the loop (to use a validation)
			#print(i,X_test, y_test)
			X_train_CVfolds = np.vstack([X_CVfolds[j] for j in np.delete(fold_index, i)]) # Returns a new X array without the array being removed in the loop
			y_train_CVfolds = np.vstack([y_CVfolds[j] for j in np.delete(fold_index, i)]) # Returns a new y array without the array being removed in the loop
			B = [
					coordinateDescent(X_train_CVfolds, np.hstack(y_train_CVfolds) , tuning_param[0], tuning_param[1]) 
					for tuning_param in tuning_params
			] # while it loops through the tuple of params it calles the elistic function and appends it to the list

			error = [
				get_loss2( np.column_stack((X_test_CVfolds, y_test_CVfolds)) , B[j] )# OR USE 
				for j in np.arange(max_Parm_iteration)
			]
			B_list = B_list + [B] # Applend the new Beta to the Beta list
			errors = errors + [error] # Applend the new error to the error list
			#par = par + [tuning_params]
	fold_means = np.mean(np.array(errors), 0)  # mean axis = 0 is column; axis = 1 is rows
	optimal_tune_index = fold_means.argmin() # finds the index of the minimum mean
	tune_star = tuning_params[optimal_tune_index] # returns the tuning parameters that had the lowest Mean fold
	return [
	errors, fold_means, fold_means[optimal_tune_index], tuning_params,tune_star 
	]

elastic_net_cv = elastic_net_GS ( Xst, y_Centered , k=5, alpha = learning_rates_Œª , l1_ratio = l1_ratio )

results_tuple  = (np.array(elastic_net_cv[3]).reshape(-1,2) , np.array(elastic_net_cv[1]).reshape(-1,1) )
SmCVMSE = pd.DataFrame( np.column_stack( results_tuple ) )
SmCVMSE.columns=[ 'alpha_Œª','l1_ratio','Avg_MSE_Training']

print('Result fo the smallet CV')
SmCVMSE[SmCVMSE.Avg_MSE_Training == SmCVMSE.Avg_MSE_Training.min()]

plt.figure(figsize=(24,12))

plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(0)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(0)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 0')
plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(0.2)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(0.2)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 0.2')
plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(0.4)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(0.4)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 0.4')
plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(0.6)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(0.6)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 0.6')
plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(0.8)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(0.8)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 0.8')
plt.plot( SmCVMSE[SmCVMSE.l1_ratio.eq(1)]["alpha_Œª"] , np.absolute( SmCVMSE[SmCVMSE.l1_ratio.eq(1)]["Avg_MSE_Training"] ), label='Avg_MSE_T alpha = 1')


#plt.plot(par,Avg_MSE_Training)
plt.title('Effect of tunning on Coefficients')
plt.xlabel('Learning Rates Œª')
plt.ylabel('Cross Validation MSE')

plt.xscale('log')
plt.legend(loc=0)

plt.show()

"""**CV Elastic Net with Library**"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import ElasticNet

#Define the model
Library_ElasticNet = ElasticNet()

# Create the Kfold:
cv_iterator = KFold(n_splits = 5, shuffle=True, random_state=101)
cv_score = cross_val_score(Library_ElasticNet, Xst, y_Centered, cv=cv_iterator, scoring='neg_mean_squared_error', n_jobs=1)
#print (cv_score)
#print ('Cv score: mean %0.3f std %0.3f' % (np.mean(np.abs(cv_score)), np.std(cv_score))) 

# define grid
Parm_grid = dict()
Parm_grid['alpha'] = learning_rates_Œª
Parm_grid['l1_ratio'] = l1_ratio

# Lets define search
GsearchCV = GridSearchCV(estimator = Library_ElasticNet, param_grid = Parm_grid, scoring = 'neg_mean_absolute_error', n_jobs=1, refit=True, cv=cv_iterator)
GsearchCV.fit(Xst, y_Centered)

GCV_df = pd.concat([pd.DataFrame(GsearchCV.cv_results_["params"]),pd.DataFrame(GsearchCV.cv_results_["mean_test_score"], columns=["mean_test_score"])],axis=1)
#GCV_df.index=GCV_df['alpha']

GCV_df[:3]

plt.figure(figsize=(25/2.54,15/2.54))

plt.plot( GCV_df[GCV_df.l1_ratio.eq(0)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0)]["mean_test_score"] ), label='Avg_MSE_T alpha = 0')

plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.2)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.2)]["mean_test_score"]), label='Avg_MSE_T alpha = 0.2')

plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.4)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.4)]["mean_test_score"]), label='Avg_MSE_T alpha = 0.4')
plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.6)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.6)]["mean_test_score"]), label='Avg_MSE_T alpha = 0.6')
plt.plot( GCV_df[GCV_df.l1_ratio.eq(0.8)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(0.8)]["mean_test_score"]), label='Avg_MSE_T alpha = 0.8')
plt.plot( GCV_df[GCV_df.l1_ratio.eq(1)]["alpha"] , np.absolute( GCV_df[GCV_df.l1_ratio.eq(1)]["mean_test_score"]), label='Avg_MSE_T alpha = 1')


plt.title('Effect of tunning on Coefficients')
plt.xlabel('Learning Rates Œª')
plt.ylabel('Cross Validation MSE')

plt.xscale('log')
plt.legend(loc=0)

plt.show()

"""# **Deliverable 6.3**  <a class="anchor" id="Deliverable_6.3"></a>
Indicate the value of ùúÜ that generated the smallest CV(5) error

**Smallest MCV with Library**
"""

print('Result fo the smallet CV MSE ', elastic_net_cv[-1])

print ('Best CV mean squared error: %0.3f' % np.abs(elastic_net_cv[-3]))

"""**Smallest CV with Library**"""

print ('Best: ',GsearchCV.best_params_)
print ('Best CV mean squared error: %0.3f' % np.abs(GsearchCV.best_score_))

GCV_df.sort_values(by=['mean_test_score'], ascending=False)[:1]

# Alternative: sklearn.linear_model.ElasticNetCV
from sklearn.linear_model import ElasticNetCV


auto_EN = ElasticNetCV(alphas=learning_rates_Œª, l1_ratio = l1_ratio, normalize=False,  n_jobs=1,  cv=cv_iterator)
auto_EN.fit(Xst, y_Centered)
print ('Best alpha: %0.5f' % auto_EN.alpha_)
print ('Best L1 ratio: %0.5f' % auto_EN.l1_ratio_)

"""# **Deliverable 6.4**  <a class="anchor" id="Deliverable_6.4"></a>
Given the optimal ùúÜ, retrain your model on the entire dataset of ùëÅ=400 observations and provide the estimates of the ùëù=9 best-fit model parameters.

**Tunned with best alpha Manually**
"""

cord_best = coordinateDescent(Xst, y_Centered , elastic_net_cv[-1][0] , elastic_net_cv[-1][1]) 

print ("Betas= ", cord_best)
y_predM_best = predict(X_test,cord_best)
print("MSE = ",mean_squared_error(y_test, y_predM_best))
print('R^2 Test', r2_score(y_test, y_predM_best))

"""**Tunned with best alpha with Library**"""

Library_ElasticNet_best=ElasticNet(alpha=GsearchCV.best_params_['alpha'] , l1_ratio= GsearchCV.best_params_['l1_ratio'], max_iter=1000, tol=0.1)
Library_ElasticNet_best.fit( Xst, y_Centered )

y_predM_best = Library_ElasticNet_best.predict(X_test)
print ("Betas= ", Library_ElasticNet_best.coef_)

print("MSE = ",mean_squared_error(y_test, y_predM_best))
print('R^2 Test', r2_score(y_test, y_predM_best))

"""# **Deliverable 6. Reason for difference**  <a class="anchor" id="Deliverable_6_Reason_for_difference"></a>"""